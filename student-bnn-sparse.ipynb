{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecbf68-455e-4d3b-878a-12faf9a9206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e241d97-d66e-43f5-a436-156b44b337c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic Relevance Determination\n",
    "\n",
    "We consider the general setting of the type-2 likelihood optimisation. Hence, we rely on the variational inference framework. In more restrictive cases, like linear models, derivations are closed form and more effective optimization routine avaliable ([example](https://github.com/evgenii-egorov/sk-bdl/blob/main/seminar_2/notebook/RVMReg-Solutions.ipynb)).\n",
    "\n",
    "Let's introduce heroes of our equations:\n",
    "+ A training dataset of target variables $y_n$ and features $x_n$ for each object $n$, that we observe: $$D=\\{x_n,y_n\\}_{n=1}^{N}.$$ \n",
    "+ A likelihood or our model assumptions on the distribution of the target, given hidden parameters $w$. \n",
    "Note that here we assume conditional independence between differen objects given $w$: $$p(D|w) = \\prod_{n=1}^{N}p(y_n|x_n, w).$$\n",
    "+ A prior distribution over hidden parameters $w$ $$p(w|\\tau).$$ \n",
    "\n",
    "The type-2 likelihood optimisation is the following routine:\n",
    "$$\n",
    "\\tau^{*} = \\arg\\max\\limits_{\\tau} \\log p(D|\\tau)= \\arg\\max\\limits_{\\tau}\\log\\int_{\\text{supp}(p)}p(w) p(D|w)\\,dw.\n",
    "$$\n",
    "\n",
    "For arbitary pair of the likelihood and prior the integral is intractable. Hence, we introduce optimisation over variational lower bound:\n",
    "$$\n",
    "\\tau^{*} = \\arg\\max\\limits_{\\tau}\\log p(D|\\tau)= \\arg\\max\\limits_{\\tau, q(w)}\\,\\int_{\\text{supp}(q)} q(w)\\log p(D|w)\\,dw - \\text{KL}[q(w);p(w|\\tau)],\\\\\n",
    "\\text{KL}[q(w);p(w|\\tau)] = -\\text{H}[q]-\\int_{\\text{supp}(q)} q(w)\\log p(w|\\tau)\\,dw.\n",
    "$$\n",
    "\n",
    "At this point we need to introduce approximation to get further:\n",
    "1. Parametrize $p(w|\\tau), p(y_n|x_n, w)$ and $q(w|\\phi)$ with suitable densities.\n",
    "2. Relax optimization in coordinate-wise manner: find optimal $\\tau$ for any $q(w|\\phi)$ as a function $\\tau(\\phi)$ and then optimize variational lower bound with respect to the $\\phi$.\n",
    "\n",
    "Let's start with the parametrisation. We have quite a freedom. However, we need to align parametrisation with our goals. We start from the assumption that the complixity flexiability of the model $p(y_n|x_n, w)$ depends from the number of non-zero $w$. Then a possible prior distirbution to reflect this is following:\n",
    "$$\n",
    "p(w|\\tau)=\\prod\\limits_{d=1}^{D}\\mathcal{N}(w_d|0,\\tau_d^{-1}), w_d\\in\\mathbb{R},\n",
    "$$\n",
    "where $d\\in\\{1,\\dots,D\\}$ is suitable indexing, i.e. we flatten $w$. Than for variational distribution we can select the same parametrization family:\n",
    "$$\n",
    "q(w|\\phi)=\\prod\\limits_{d=1}^{D}\\mathcal{N}(w_d|\\mu_d,\\sigma_d^2), w_d\\in\\mathbb{R}.\n",
    "$$\n",
    "\n",
    "Aimed with this let's go to the step 2.\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\phi}\\max\\limits_{\\tau(\\phi)}\\,\\int_{\\text{supp}(q)} q(w)\\log p(D|w)\\,dw - \\text{KL}[q(w);p(w|\\tau)] = \\max\\limits_{\\phi}\\,\\int_{\\text{supp}(q)} q(w|\\phi)\\log p(D|w)\\,dw + \\max\\limits_{\\tau(\\phi)} - \\text{KL}[q(w|\\phi);p(w|\\tau)]. \n",
    "$$\n",
    "\n",
    "Let's solve internal loop:\n",
    "\n",
    "$$\n",
    "\\max\\limits_{\\tau(\\phi)}- \\text{KL}[q(w|\\phi);p(w|\\tau)] = \\min\\limits_{\\tau(\\mu,\\sigma)}\\sum\\limits_{d=1}^{D}\\text{KL}\\left[\\mathcal{N}(w_d|\\mu_d,\\sigma_d^2);\\mathcal{N}(w_d|0,\\tau_d^{-1})\\right].\n",
    "$$\n",
    "By first-order condition:\n",
    "$$\n",
    "\\tau^{*}(\\mu,\\sigma)_d = \\left(\\mu_d^2 + \\sigma^2_d \\right)^{-1}.\n",
    "$$\n",
    "Substitute back, we obtain:\n",
    "$$\n",
    "\\phi^{*} = \\arg\\max\\limits_{\\phi}\\max\\limits_{\\tau(\\phi)}\\,\\int_{\\text{supp}(q)} q(w)\\log p(D|w)\\,dw - \\text{KL}[q(w);p(w|\\tau)] =  \\\\ \n",
    "= \\arg\\max\\limits_{\\phi}\\,\\int_{\\text{supp}(q)} q(w|\\phi)\\log p(D|w)\\,dw -\\sum\\limits_{d=1}^{D}\\log\\left(1+\\dfrac{\\mu_d^2}{\\sigma^2_d}\\right). \n",
    "$$\n",
    "\n",
    "We can also re-write in following parametrisation:\n",
    "$$\n",
    "\\alpha_d = \\left(\\dfrac{\\sigma_d^2}{\\mu^2_d}\\right), \\mathcal{N}(w_d|\\mu_d,\\sigma_d^2) = \\mathcal{N}(w_d|\\mu_d,\\mu_d^2\\alpha_d).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3e058-0e25-44d5-b95b-b3f72b3bc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearStoch(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(LinearStoch, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        self.W = # your code here\n",
    "        self.log_std = # your code here\n",
    "        self.bias = # your code here\n",
    "        \n",
    "        self.init_params(std=1e-2)\n",
    "        \n",
    "    def init_params(self, std):\n",
    "        self.bias.data.zero_()\n",
    "        self.W.data.normal_(0, std)\n",
    "        self.log_std.data.fill_(-3)\n",
    "        \n",
    "    def get_mask(self, alpha_th=3):\n",
    "        log_alpha = # your code here\n",
    "        mask = (log_alpha < alpha_th).float()\n",
    "        return log_alpha, mask\n",
    "    \n",
    "    def kl_term(self):\n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, x, non_stoch=False):\n",
    "        train = self.training \n",
    "        \n",
    "        if non_stoch:\n",
    "            stoch_part = 0.\n",
    "            activation_mean = F.linear(x, self.W) + self.bias\n",
    "            return activation_mean\n",
    "        \n",
    "        if train:\n",
    "            activation_var = # your code here\n",
    "            activation_mean = # your code here\n",
    "            stoch_part = torch.sqrt(activation_var + 1e-6) * torch.randn_like(activation_mean)\n",
    "        else:\n",
    "            stoch_part = 0.\n",
    "            log_alpha, mask = self.get_mask()\n",
    "            activation_mean = F.linear(x, self.W * mask) + self.bias\n",
    "            \n",
    "        return activation_mean + stoch_part\n",
    "    \n",
    "class ResF(nn.Module):\n",
    "    def __init__(self, F):\n",
    "        super(ResF, self).__init__()\n",
    "        self.F = F\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.F(x)\n",
    "    \n",
    "    \n",
    "class ARD(nn.Module):\n",
    "    def __init__(self, net, train_size):\n",
    "        super(ARD, self).__init__()\n",
    "        self.train_size = train_size\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, input, target, kl_weight=1.0):\n",
    "        kl = 0.0\n",
    "        for module in self.net.children():\n",
    "            if hasattr(module, 'kl_term'):\n",
    "                kl = kl + module.kl_term()\n",
    "        return F.mse_loss(input, target, reduction='mean') * self.train_size + kl_weight * kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025f981-359d-4824-8ad0-cc462f198d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_net():\n",
    "    d_in = 1\n",
    "    d_out = 1\n",
    "    h = 150\n",
    "    depth_r = 0\n",
    "    depth_nr = 3\n",
    "\n",
    "    net = nn.Sequential(\n",
    "        LinearStoch(d_in, h),\n",
    "        nn.Tanh(),\n",
    "\n",
    "        *[ResF(\n",
    "            nn.Sequential(LinearStoch(h, h), nn.Tanh())\n",
    "            ) for d in range(depth_r)],\n",
    "\n",
    "        *[nn.Sequential(LinearStoch(h, h), nn.Tanh())\n",
    "             for d in range(depth_nr)],\n",
    "        LinearStoch(h, d_out)\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664aa59-f838-4907-8398-300fa9a9464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Numpy seed\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(-.2, 0.2, 500)\n",
    "x = np.hstack([x, np.linspace(.6, 1, 500)])\n",
    "ε = 0.02 * np.random.randn(x.shape[0])\n",
    "y = x + 0.3 * np.sin(2 * np.pi * (x + ε)) + 0.3 * np.sin(4 * np.pi * (x + ε)) + ε\n",
    "\n",
    "x_true = np.linspace(-.5, 1.5, 1000)\n",
    "y_true = x_true + 0.3 * np.sin(2 * np.pi * x_true) + 0.3 * np.sin(4 * np.pi * x_true) \n",
    "\n",
    "xlims = [-.5, 1.5]\n",
    "ylims = [-1.5, 2.5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(ylims)\n",
    "plt.xlabel(\"X\", fontsize=30)\n",
    "plt.ylabel(\"Y\", fontsize=30)\n",
    "\n",
    "ax.plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n",
    "ax.plot(x, y, 'ko', markersize=4, label=\"observations\")\n",
    "\n",
    "plt.legend(loc=4, fontsize=15, frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798aa800-35d3-4a10-84f3-136178bad9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x.shape[0]\n",
    "x_train = torch.from_numpy(x).float()\n",
    "y_train = torch.from_numpy(y).float()\n",
    "\n",
    "x_true = torch.from_numpy(x_true).float()\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=128, shuffle=True)\n",
    "net = make_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68472fcb-9db9-4d62-8d2e-85cfd88b851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, kl_weight, N):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "    ard_loss = ARD(net, N).cuda()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        train_loss, train_mse = 0, 0 \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            x = data.cuda().unsqueeze(-1)\n",
    "            y = target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = net(x).flatten()\n",
    "\n",
    "            loss = ard_loss(output, y, kl_weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            train_loss += loss.item() \n",
    "            train_mse += torch.sum((y - output) ** 2).item()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print((train_loss-train_mse) / N, train_mse / N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9b47f-2f2d-4ffe-9cc0-a08cdf52dc0f",
   "metadata": {},
   "source": [
    "Here we train stochasticly, but without $KL$ term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd026a-0307-4b50-9a5b-d9e64cd616ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(900, 0, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61c9ab-ed00-4706-a732-24aff534c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    xlims = [-.5, 1.5]\n",
    "    ylims = [-1.5, 2.5]\n",
    "\n",
    "    K = 4\n",
    "    fig, ax = plt.subplots(1, K, figsize=(25, 5))\n",
    "    ax[0].set_xlim(xlims)\n",
    "    ax[0].set_ylim(ylims)\n",
    "    ax[0].set_xlabel(\"X\", fontsize=30)\n",
    "    ax[0].set_ylabel(\"Y\", fontsize=30)\n",
    "\n",
    "    ax[0].plot(x_true, y_true, 'b-', linewidth=3, label=\"true function\")\n",
    "    ax[0].plot(x, y, 'ko', markersize=4, label=\"observations\")\n",
    "\n",
    "    ax[0].legend(loc=4, fontsize=15, frameon=False);\n",
    "    net.eval()\n",
    "    ax[0].plot(x_true, net(x_true.cuda().unsqueeze(-1)).flatten().cpu().detach(), color='red', label='prediction');\n",
    "    ax[0].grid(True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ax[1].imshow(torch.abs(net[0].W.cpu()))\n",
    "        ax[1].axis('off')\n",
    "        ax[1].set_title('weights')\n",
    "        \n",
    "        for i in (2,3):\n",
    "            _, mask = net[i][0].get_mask()\n",
    "            ax[i].imshow(torch.abs(net[i][0].W.cpu() * mask.cpu()))\n",
    "            ax[i].axis('off')\n",
    "            ax[i].set_title('weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6c2a4-d02c-4f74-a0c8-c5f702db16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb008c0d-a190-4bb1-9d06-e35271ede91e",
   "metadata": {},
   "source": [
    "Let's add $KL$ with weight $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9cf02-bd88-409a-9da1-4d4dd57e77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(900, 1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a67be4-d80b-4bbf-b182-99c75aedaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0518a9-66d5-4aac-94cc-0a45e24d745d",
   "metadata": {},
   "source": [
    "Let's do something in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee8e19-819f-4fae-a775-9a5462feb18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = make_net().cuda()\n",
    "train(900, 0, N)\n",
    "train(100, 0.1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf578e1-5727-48a6-8943-f619c46053b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
